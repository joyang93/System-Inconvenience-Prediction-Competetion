{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acQ7cOOn81Cl"
   },
   "source": [
    "\n",
    "## 1. 모듈 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ObuFqMya65p8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 필요한 함수 정의\n",
    "def make_datetime(x):\n",
    "    # string 타입의 Time column을 datetime 타입으로 변경\n",
    "    x     = str(x)\n",
    "    year  = int(x[:4])\n",
    "    month = int(x[4:6])\n",
    "    day   = int(x[6:8])\n",
    "    hour  = int(x[8:10])\n",
    "    #mim  = int(x[10:12])\n",
    "    #sec  = int(x[12:])\n",
    "    return dt.datetime(year, month, day, hour)\n",
    "\n",
    "def string2num(x):\n",
    "    # (,)( )과 같은 불필요한 데이터 정제\n",
    "    x = re.sub(r\"[^0-9]+\", '', str(x))\n",
    "    if x =='':\n",
    "        return 0\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def clean_quality(df):\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "        'quality_5', 'quality_6', 'quality_7',\n",
    "      'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "  for col in cols:\n",
    "    \n",
    "    if df[col].dtype == object:\n",
    "      df[col] = df[col].apply(lambda x : 0 if ((type(x) is str) and (x.count('.') >= 2)) else x)\n",
    "      \n",
    "      df[col] = df[col].astype(str).str.replace(',','')\n",
    "      # df[col] = df[col].apply(lambda x : 0 if type(x) is str and x.count('.') >= 2 else x)\n",
    "\n",
    "    df[col] = df[col].astype(float)\n",
    "  return df\n",
    "\n",
    "def convert_unixtime(date_time):\n",
    "  import datetime\n",
    "  unixtime = datetime.datetime.strptime(date_time,\n",
    "                              '%Y%m%d%H%M%S').timestamp()\n",
    "  return unixtime\n",
    "    \n",
    "RAW_PATH = '../data/raw/'\n",
    "FEATURE_PATH = '../data/final/'\n",
    "INTERMEDIATE_PATH = '../data/intermediate/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwW1PcWL88XT"
   },
   "source": [
    "## 2. 데이터셋 불러오기, 기본 데이터 전처리\n",
    "> 다음 셀 블록에서 전처리 완료된 pkl file을 로드 하므로 주석처리함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PBAABnc7GpW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # train, test 에러, 퀄리티 데이터 불러오기\n",
    "# train_err  = reduce_mem_usage(pd.read_csv(RAW_PATH+'train_err_data.csv'))\n",
    "# test_err  = reduce_mem_usage(pd.read_csv(RAW_PATH+'test_err_data.csv'))\n",
    "\n",
    "# # # 에러타입+코드를 붙인 파생변수 생성, 길이는 5자로 제한\n",
    "# train_err[\"type_code\"] = train_err['errtype'].astype(str) + \"_\" + train_err['errcode'].astype(str)\n",
    "# test_err[\"type_code\"] = test_err['errtype'].astype(str) + \"_\" + test_err['errcode'].astype(str)\n",
    "# train_err[\"type_code\"] = list(map(lambda x: x[:5], train_err.type_code.values))\n",
    "# test_err[\"type_code\"] = list(map(lambda x: x[:5], test_err.type_code.values))\n",
    "\n",
    "# # type_code와 error code에 대한 사전 생성 \n",
    "# union_type_list = list(set(train_err.type_code).union(set(test_err.type_code)))\n",
    "# type_code_dic = dict(zip(union_type_list, range(len(union_type_list))))\n",
    "# train_err['errcode'] = train_err.errcode.astype(str).apply(lambda x: x[:3])\n",
    "# test_err['errcode'] = test_err.errcode.astype(str).apply(lambda x: x[:3])\n",
    "# errcode_set = list(set(test_err.errcode).union(set(train_err.errcode)))\n",
    "# errcode_dic = dict(zip(errcode_set, range(len(errcode_set))))\n",
    "\n",
    "# with open(INTERMEDIATE_PATH+'type_code_dic', 'w') as file:\n",
    "#     json.dump(type_code_dic, file)\n",
    "\n",
    "# with open(INTERMEDIATE_PATH+'errcode_dic', 'w') as file:\n",
    "#     json.dump(errcode_dic, file)\n",
    "\n",
    "\n",
    "\n",
    "# # train_err['time'] = train_err.time.astype(str)\n",
    "# # test_err['time'] = test_err.time.astype(str)\n",
    "\n",
    "# # train_err[\"timestamp\"] = list(map(lambda x: convert_unixtime(str(x)), train_err.time.values))\n",
    "# # test_err[\"timestamp\"] = list(map(lambda x: convert_unixtime(str(x)), test_err.time.values))\n",
    "# # train_err[\"time_day\"] = list(map(lambda x : str(x)[:8], train_err.time.values))\n",
    "# # train_err[\"time_hour\"] = list(map(lambda x : str(x)[:10], train_err.time.values))\n",
    "# # test_err[\"time_day\"] = list(map(lambda x : str(x)[:8], test_err.time.values))\n",
    "# # test_err[\"time_hour\"] = list(map(lambda x : str(x)[:10], test_err.time.values))\n",
    "# train_err.to_pickle(INTERMEDIATE_PATH + \"train_err_cplt.pkl\")\n",
    "# test_err.to_pickle(INTERMEDIATE_PATH + \"test_err_cplt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33951,
     "status": "ok",
     "timestamp": 1612271742438,
     "user": {
      "displayName": "yang jo",
      "photoUrl": "",
      "userId": "01079575632044760993"
     },
     "user_tz": -540
    },
    "id": "CSsgvU6YfesI",
    "outputId": "5cadbf46-0966-41e3-c1bd-06fc1c0e9f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 678.87 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 709.50 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "# 위의 전처리 과정 완료된 결과물 바로 불러오기 \n",
    "train_err = reduce_mem_usage(pd.read_pickle(INTERMEDIATE_PATH + \"train_err_cplt.pkl\"))\n",
    "test_err = reduce_mem_usage(pd.read_pickle(INTERMEDIATE_PATH  + \"test_err_cplt.pkl\"))\n",
    "\n",
    "train_quality  = (clean_quality(pd.read_csv(RAW_PATH+'train_quality_data.csv')))\n",
    "test_quality  = (clean_quality(pd.read_csv(RAW_PATH+'test_quality_data.csv')))\n",
    "\n",
    "all_day_list = ['20201031', '20201101', '20201102', '20201103','20201104', '20201105', '20201106', '20201107', '20201108', '20201109', '20201110',\n",
    " '20201111', '20201112', '20201113', '20201114', '20201115', '20201116', '20201117', '20201118', '20201119', '20201120', '20201121', '20201122',\n",
    " '20201123', '20201124', '20201125', '20201126', '20201127', '20201128', '20201129', '20201130', '20201201', '20201202']\n",
    "\n",
    "with open(INTERMEDIATE_PATH + 'type_code_dic', 'r') as file:\n",
    "    type_code_dic = json.load(file)\n",
    "with open(INTERMEDIATE_PATH + 'errcode_dic', 'r') as file:\n",
    "    errcode_dic = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcQa2ENf9FhH"
   },
   "source": [
    "## 3. 피쳐엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7w9noBP9t4Z"
   },
   "source": [
    "### 에러 발생 시간에 따른 분포에 대한 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMXhBGA15jxW"
   },
   "outputs": [],
   "source": [
    "def get_type_code_timestamp_features(df,train_yn):\n",
    "  '''\n",
    "  유저별, 에러 타입 + 코드 별, 발생한 timestamp를 구하고 그에 따른 통계량(mean, std, min, max, 최대 최소 시간간격, 카운트 수/최대 최소 시간간격)을 추출하는 함수\n",
    "  * 타입코드별 전체 기간에 대한 시간 분포 통계량 \n",
    "  '''\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "\n",
    "  df[\"timestamp\"] = df.time.apply(lambda x : convert_unixtime(str(x)))\n",
    "  rst_df = df.groupby([\"user_id\",\"type_code\"]).timestamp.describe()\n",
    "  rst_df['time_min_max_interval'] = rst_df['max'] - rst_df['min']\n",
    "  rst_df['count/time_min_max_interval'] = rst_df['count']/rst_df['time_min_max_interval']\n",
    "  rst_df = rst_df.replace([np.inf, -np.inf], np.nan)\n",
    "  rst_df = rst_df.reset_index().fillna(0)[['user_id','type_code','mean','std','min','max','time_min_max_interval', 'count/time_min_max_interval']]\n",
    "\n",
    "  source_df[\"type_code\"] =[list(type_code_dic.keys())]*len(source_df)\n",
    "  source_df = source_df.explode(\"type_code\")\n",
    "  source_df = pd.merge(source_df, rst_df, on =  [\"user_id\",\"type_code\"], how = 'left').fillna(0)\n",
    "  return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WtKErYMw_uw"
   },
   "outputs": [],
   "source": [
    "train_ty_cd_timestamp_feature = get_type_code_timestamp_features(train_err, True)\n",
    "test_ty_cd_timestamp_feature = get_type_code_timestamp_features(test_err,False)\n",
    "\n",
    "train_ty_cd_timestamp_feature.to_pickle(FEATURE_PATH + \"train_ty_cd_timestamp_feature.pkl\")\n",
    "test_ty_cd_timestamp_feature.to_pickle(FEATURE_PATH + \"test_ty_cd_timestamp_feature.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiYmwc3u9zob"
   },
   "source": [
    "### 에러 로그 카운트 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627165,
     "status": "ok",
     "timestamp": 1612233756070,
     "user": {
      "displayName": "yang jo",
      "photoUrl": "",
      "userId": "01079575632044760993"
     },
     "user_tz": -540
    },
    "id": "e7ztO6bDR7Wu",
    "outputId": "c964d1a3-fe67-4c79-cbb9-28d51d5d1709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get derived variable  type_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16554663/16554663 [00:35<00:00, 471305.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (15000, 259)\n",
      "rst_error shape : (15000, 259)\n",
      "get derived variable  errcode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16554663/16554663 [00:34<00:00, 473706.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (15000, 922)\n",
      "rst_error shape : (15000, 1181)\n",
      "get derived variable  model_nm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16554663/16554663 [00:39<00:00, 423228.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (15000, 9)\n",
      "rst_error shape : (15000, 1190)\n",
      "get derived variable  errtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16554663/16554663 [01:57<00:00, 140451.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (15000, 42)\n",
      "rst_error shape : (15000, 1232)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocessing_static(df):\n",
    "  '''\n",
    "  유저별 퀄리티 컬럼 별 통계치(max, min, std, quanitle(0.75,0.5,0.25)) 추출 함수 \n",
    "  '''\n",
    "  rst_df = df.groupby(\"user_id\").count().reset_index()[[\"user_id\"]]\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "        'quality_5', 'quality_6', 'quality_7',\n",
    "      'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "  for col in cols:\n",
    "    tmp = df.groupby(\"user_id\")[col].describe().reset_index() \n",
    "    tmp.columns = ['user_id'] + [col + \"_\" + i for i in tmp.columns[1:]]\n",
    "    rst_df = pd.concat([rst_df, tmp.iloc[:,1:]], axis = 1)\n",
    "  return rst_df\n",
    "\n",
    "def get_derived_variable(df, cols, type_code_dic, errcode_dic, train_yn):\n",
    "  '''\n",
    "  에러 타입, 모델명, 에러 코드 , 에러 타입+코드별 카운트 집계 함수\n",
    "  '''\n",
    "  # df['errtype'] = df['errtype'].astype(int)\n",
    "\n",
    "  if train_yn == True:\n",
    "    user_number = 15000\n",
    "    user_id_min = 10000\n",
    "  else:\n",
    "    user_number = 14999\n",
    "    user_id_min = 30000\n",
    "\n",
    "  rst_error = '1'\n",
    "  for col in cols:\n",
    "    print(\"get derived variable \", col)\n",
    "    length = len(list(set(train_err[col]).union(set(test_err[col]))) ) \n",
    "    id_error = df[['user_id', col]].values\n",
    "    error = np.zeros((user_number, length))\n",
    "\n",
    "    if col == \"errtype\":   \n",
    "      error = np.zeros((user_number, 42)) # 29번이 없음\n",
    "\n",
    "    for person_idx, err in tqdm(id_error):\n",
    "      if col == \"type_code\":\n",
    "        number = type_code_dic[err]\n",
    "        error[person_idx - user_id_min, number - 1] += 1\n",
    "      \n",
    "      if col == \"errcode\":\n",
    "        number = errcode_dic[err]\n",
    "        error[person_idx - user_id_min, number - 1] += 1\n",
    "      \n",
    "      if col == \"model_nm\": \n",
    "        number = int(err[-1]) # 모델 끝번호  \n",
    "        error[person_idx - user_id_min, number - 1] += 1\n",
    "\n",
    "      if col == \"errtype\":   \n",
    "        error[person_idx - user_id_min, err - 1] += 1 \n",
    "\n",
    "    if len(rst_error) == 1:\n",
    "      rst_error = error\n",
    "      print(\"error shape :\", error.shape)\n",
    "      print(\"rst_error shape :\", rst_error.shape)\n",
    "    else:\n",
    "      rst_error = np.append(rst_error, error, axis = 1)\n",
    "      print(\"error shape :\", error.shape)\n",
    "      print(\"rst_error shape :\", rst_error.shape)\n",
    "  \n",
    "  return rst_error\n",
    "def get_timestamp_features(df):\n",
    "  #time에 대한 max-min\n",
    "  #time에 대한 분산 \n",
    "  def convert_unixtime(date_time):\n",
    "    \"\"\"Convert datetime to unixtime\"\"\"\n",
    "    import datetime\n",
    "    unixtime = datetime.datetime.strptime(date_time,\n",
    "                               '%Y%m%d%H%M%S').timestamp()\n",
    "    return unixtime\n",
    "  df[\"timestamp\"] = df.time.apply(lambda x : convert_unixtime(str(x)))\n",
    "  rst_df = df.groupby(\"user_id\").timestamp.describe()[['std','min','max']].reset_index()\n",
    "  rst_df['time_min_max_interval'] = rst_df['max'] - rst_df['min']\n",
    "  \n",
    "  del rst_df['max']\n",
    "  del rst_df['min']\n",
    "  \n",
    "  rst_df.columns = [\"user_id\",\"timestamp_std\",\"time_min_max_interval\"]\n",
    "  return rst_df\n",
    "\n",
    "\n",
    "cols  = [\"type_code\",\"errcode\",\"model_nm\",\"errtype\"]\n",
    "error_train = get_derived_variable(train_err, cols, type_code_dic, errcode_dic, True)\n",
    "rst_df = preprocessing_static(train_quality)\n",
    "source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "source_df = pd.merge(source_df, rst_df, on = [\"user_id\"], how = 'left').fillna(0)\n",
    "\n",
    "train_err_time_agg = get_timestamp_features(train_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1249211,
     "status": "ok",
     "timestamp": 1612234386625,
     "user": {
      "displayName": "yang jo",
      "photoUrl": "",
      "userId": "01079575632044760993"
     },
     "user_tz": -540
    },
    "id": "x8BKvgBuOON3",
    "outputId": "24d8e444-7fec-4517-dca9-691286fe9011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get derived variable  type_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16532648/16532648 [00:35<00:00, 464721.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (14999, 259)\n",
      "rst_error shape : (14999, 259)\n",
      "get derived variable  errcode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16532648/16532648 [00:35<00:00, 462818.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (14999, 922)\n",
      "rst_error shape : (14999, 1181)\n",
      "get derived variable  model_nm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16532648/16532648 [00:38<00:00, 431163.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (14999, 9)\n",
      "rst_error shape : (14999, 1190)\n",
      "get derived variable  errtype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16532648/16532648 [02:00<00:00, 137138.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error shape : (14999, 42)\n",
      "rst_error shape : (14999, 1232)\n"
     ]
    }
   ],
   "source": [
    "source_df = pd.merge(source_df, train_err_time_agg, on = [\"user_id\"], how = 'left').fillna(0)\n",
    "error_train = np.append(error_train, source_df.iloc[:,1:].values, axis=1)\n",
    "\n",
    "np.save(FEATURE_PATH + \"train_df_err_cnt.npy\", error_train)\n",
    "\n",
    "error_test = get_derived_variable(test_err, cols, type_code_dic, errcode_dic, False)\n",
    "\n",
    "rst_df = preprocessing_static(test_quality)\n",
    "source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "source_df = pd.merge(source_df, rst_df, on = [\"user_id\"], how = 'left').fillna(0) # 퀄리티 통계량 \n",
    "test_err_time_agg = get_timestamp_features(test_err) # time stamp\n",
    "source_df = pd.merge(source_df, test_err_time_agg, on = [\"user_id\"], how = 'left').fillna(0) \n",
    "error_test = np.append(error_test, source_df.iloc[:,1:].values, axis=1) # 더미변수에 붙임 \n",
    "np.save(FEATURE_PATH + \"test_df_err_cnt.npy\", error_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn3OE1UXfhgc"
   },
   "source": [
    "### 에러 로그 reg 피쳐\n",
    "> 유저별 타입코드별 ,시간별 추세에 대한 회귀직선 값 도출(알파, 베타)*259타입코드수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn45-OBXIK0L"
   },
   "outputs": [],
   "source": [
    "def get_typecode_reg(df,  train_yn):\n",
    "  # 유저별 타입 코드별 일별 카운트 수에 대한 추세(alpha, beta) 2dim * 타입수(259)\n",
    "\n",
    "  import statsmodels.api as sm\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "    \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "\n",
    "  df['errtype'] = df['errtype'].astype(str)\n",
    "\n",
    "  df[\"time_day\"] = list(map(lambda x : str(x)[:8], df.time.values))\n",
    "\n",
    "  tmp = df.groupby([\"user_id\",\"type_code\",\"time_day\"]).count()[['time']].reset_index().sort_values([\"user_id\",\"type_code\",'time_day'])\n",
    "  tmp.columns = [\"user_id\",\"type_code\",\"time_day\",\"cnt\"]\n",
    "  tmp_ = tmp.groupby([\"user_id\",\"type_code\"]).cnt.apply(list).reset_index()\n",
    "  tmp_['time_idx'] = tmp_.cnt.apply(lambda x : [i for i in range(len(x))])\n",
    "  rst = tmp_[['user_id','type_code']]\n",
    "  reg_coef_list = list(map(lambda x: sm.OLS(x[0],sm.add_constant(x[1])).fit().params, tmp_[['cnt','time_idx']].values))\n",
    "  reg_alpha_list = list(map(lambda x: x[0], reg_coef_list))\n",
    "  reg_beta_list = list(map(lambda x: x[1], reg_coef_list))\n",
    "  rst['reg_alpha_list'] = reg_alpha_list\n",
    "  rst['reg_beta_list'] = reg_beta_list\n",
    "\n",
    "  source_df[\"type_code\"] =[list(type_code_dic.keys())]*len(source_df)\n",
    "  source_df = source_df.explode(\"type_code\")\n",
    "  source_df = pd.merge(source_df, rst, on = [\"user_id\",\"type_code\"], how = 'left').fillna(0)\n",
    "  return source_df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lGsj_NHIOx-"
   },
   "outputs": [],
   "source": [
    "train_typecode_reg = get_typecode_reg(train_err, True)\n",
    "test_typecode_reg = get_typecode_reg(test_err, False)\n",
    "\n",
    "train_typecode_reg.to_pickle(FEATURE_PATH + \"train_typecode_reg.pkl\")\n",
    "test_typecode_reg.to_pickle(FEATURE_PATH + \"test_typecode_reg.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsbcA_C9Wbpw"
   },
   "source": [
    "### 에러간의 time delta 분포 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4y98vKbvxOO"
   },
   "outputs": [],
   "source": [
    "def get_err_timedelta_static(df, train_yn):\n",
    "  '''\n",
    "  유저별 errtype 별 time_diff를 구하고 time_diff의 통계 피쳐를 뽑는 함수\n",
    "  '''\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "  df['errtype'] = df['errtype'].astype(str)\n",
    "  df['datetime'] = df.time.apply(lambda x: make_datetime(x))\n",
    "  df[\"timestamp\"] = df.time.apply(lambda x : convert_unixtime(str(x)))\n",
    "\n",
    "  df.set_index([\"user_id\",\"errtype\",\"datetime\"], inplace=True)\n",
    "  df.sort_index(inplace=True)\n",
    "  df['diffs'] = np.nan\n",
    "  idx = pd.IndexSlice\n",
    "  import tqdm\n",
    "  for ix in tqdm.tqdm(df.index.levels[0]):  # user_id 에 해당하는 level index 번호 필요\n",
    "    df.loc[idx[ix,:], 'diff'] = df.loc[idx[ix,:], 'timestamp'].diff()\n",
    "\n",
    "  df_ = df.reset_index()[['user_id','errtype','diff']]\n",
    "  df_ = df_.dropna() # 앞자리는 time diff 없기 때문에 제거\n",
    "  df_[\"diff\"] = df_['diff'].values\n",
    "  df_[\"diff\"]  = df_[\"diff\"].apply(lambda  x : x if x > 0 else 0)\n",
    "  df_['diff_val'] = df_['diff']\n",
    "  def f3(x):\n",
    "      x = list(x[\"diff_val\"])\n",
    "      return pd.Series({'std': np.std(x), 'max': np.max(x), \"min\": np.min(x), \"mean\": np.mean(x)}, index=[\"std\",\"max\",\"min\",\"mean\"])\n",
    "  df_agg = df_.groupby([\"user_id\",\"errtype\"]).apply(f3).reset_index()\n",
    "  errtypes_list = df_['errtype'].unique().tolist()\n",
    "  \n",
    "  source_df[\"errtype\"] = [errtypes_list]*len(source_df)\n",
    "\n",
    "  source_df = source_df.explode(\"errtype\")\n",
    "  source_df.columns = [\"user_id\", \"errtype\"]\n",
    "  source_df = pd.merge(source_df, df_agg, on = [\"user_id\",\"errtype\"], how = 'left')\n",
    "  source_df = source_df.fillna(0)\n",
    "  return np.reshape(source_df.iloc[:,2:].values, [user_size, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timedelta_tp_cd_static = get_err_timedelta_static(train_err, True)\n",
    "np.save(FEATURE_PATH + \"train_timedelta_static.npy\", train_timedelta_tp_cd_static)\n",
    "test_timedelta_tp_cd_static = get_err_timedelta_static(test_err, False)\n",
    "np.save(FEATURE_PATH + \"test_timedelta_static.npy\",test_timedelta_tp_cd_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUj2YNmU9671"
   },
   "source": [
    "### 불만 제기율 피쳐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXssvZf-J2Ra"
   },
   "outputs": [],
   "source": [
    "def get_complain_ratio_model(df, train_prob, train_yn):\n",
    "  '''\n",
    "  해당 모델의 불만 제기율, 더미 변수 추출\n",
    "  추출결과를 user_id로 조인\n",
    "  '''\n",
    "  source_df = train_prob.groupby(\"user_id\").count().reset_index()\n",
    "  \n",
    "  # 최빈값 더미 변수화\n",
    "  train_freq_model = df.groupby(['user_id'])['model_nm'].agg(pd.Series.mode).to_frame().reset_index()\n",
    "  # 최빈값이 여러개인 경우 가장 마지막 사용 모델을 선택\n",
    "  train_freq_model['model_nm'] = train_freq_model.model_nm.apply(lambda x: x[-1] if len(x) < 7 else x)\n",
    "  df_get_dummies = pd.get_dummies(train_freq_model, columns = ['model_nm'])\n",
    "  \n",
    "  # 모델별 유저수 추정\n",
    "  train_all_model_user =train_freq_model.groupby(\"model_nm\").count().reset_index()\n",
    "  train_all_model_user.columns = [\"model_nm\",\"all_cnt\"]\n",
    "\n",
    "  # 불만을 제기한 유저 데이터를 통해 모델별 불만제기수를 추정\n",
    "  # user_id, 불만 cnt, 최빈 모델\n",
    "  train_model_complain_cnt = pd.merge(source_df,  train_freq_model, on ='user_id', how = 'left')\n",
    "  train_model_complain_cnt.columns = [\"user_id\",\"complain_cnt\",\"model_nm\"]\n",
    "  train_model_complain_cnt_ = train_model_complain_cnt.groupby(\"model_nm\").complain_cnt.sum().reset_index()\n",
    "  train_model_complain_cnt_.columns = [\"model_nm\",\"all_complain_cnt\"]\n",
    "  rst_model_complain_ratio = pd.merge(train_model_complain_cnt_, train_all_model_user, on = [\"model_nm\"], how = 'left')\n",
    "  rst_model_complain_ratio['model_complain_ratio'] = rst_model_complain_ratio['all_complain_cnt']/rst_model_complain_ratio['all_cnt']\n",
    "  rst_model_complain_ratio = rst_model_complain_ratio[['model_nm','model_complain_ratio']] \n",
    "\n",
    "  # 추출 변수 병합 \n",
    "  train_freq_model = pd.merge(train_freq_model, df_get_dummies, on = 'user_id', how = 'left')\n",
    "\n",
    "  if train_yn == True:\n",
    "    return train_freq_model, rst_model_complain_ratio\n",
    "  else:\n",
    "    return train_freq_model\n",
    "\n",
    "def get_complain_ratio_fw(df, train_prob, train_yn): \n",
    "  '''해당 펌웨어의 불만 제기율, 더미 변수 추출\n",
    "  추출결과를 user_id로 조인\n",
    "  '''\n",
    "  \n",
    "  source_df = train_prob.groupby(\"user_id\").count().reset_index()\n",
    "  \n",
    "  # 최빈값 더미 변수화\n",
    "  train_freq_model = df.groupby(['user_id'])['fwver'].agg(pd.Series.mode).to_frame().reset_index()\n",
    "  # 최빈값이 여러개인 경우 가장 마지막 사용 fw을 선택\n",
    "  train_freq_model['fwver'] = train_freq_model.fwver.apply(lambda x:  x if type(x) is str else x[-1])\n",
    "\n",
    "  df_get_dummies = pd.get_dummies(train_freq_model, columns = ['fwver'])\n",
    "  \n",
    "  # 모델별 유저수 추정\n",
    "  train_all_model_user =train_freq_model.groupby(\"fwver\").count().reset_index()\n",
    "  train_all_model_user.columns = [\"fwver\",\"all_cnt\"]\n",
    "\n",
    "  # 불만을 제기한 유저 데이터를 통해 모델별 불만제기수를 추정\n",
    "  # user_id, 불만 cnt, 최빈 fwver\n",
    "  train_model_complain_cnt = pd.merge(source_df,  train_freq_model, on ='user_id', how = 'left')\n",
    "  train_model_complain_cnt.columns = [\"user_id\",\"complain_cnt\",\"fwver\"]\n",
    "  train_model_complain_cnt_ = train_model_complain_cnt.groupby(\"fwver\").complain_cnt.sum().reset_index()\n",
    "  train_model_complain_cnt_.columns = [\"fwver\",\"all_complain_cnt\"]\n",
    "  rst_model_complain_ratio = pd.merge(train_model_complain_cnt_, train_all_model_user, on = [\"fwver\"], how = 'left')\n",
    "  rst_model_complain_ratio['fw_complain_ratio'] = rst_model_complain_ratio['all_complain_cnt']/rst_model_complain_ratio['all_cnt']\n",
    "  rst_model_complain_ratio = rst_model_complain_ratio[['fwver','fw_complain_ratio']] \n",
    "\n",
    "  # 추출 변수 병합 \n",
    "  train_freq_model = pd.merge(train_freq_model, df_get_dummies, on = 'user_id', how = 'left')\n",
    "  if train_yn == True:\n",
    "    return train_freq_model, rst_model_complain_ratio\n",
    "  else:\n",
    "    return train_freq_model\n",
    "\n",
    "def get_model_fw_unique_cnt(df, train_yn):\n",
    "  '''\n",
    "  사용한 펌웨어 종류수, 모델 수 , 모델별 펌웨어 종류수\n",
    "  추출결과를 user_id로 조인\n",
    "  '''\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "\n",
    "  # 사용자별 펌웨어 unique 수\n",
    "  fwver_cnt_df = df.groupby(\"user_id\").fwver.unique().apply(len).reset_index()\n",
    "  fwver_cnt_df.columns = [\"user_id\",\"fwver_cnt\"]\n",
    "  # 사용자별 모델 unique 수 \n",
    "  model_cnt_df = df.groupby(\"user_id\").model_nm.unique().apply(len).reset_index()\n",
    "  model_cnt_df.columns = [\"user_id\",\"model_cnt\"]\n",
    "  # 사용자별 한 모델안에서 펌웨어를 얼마나 많이 교체 했는지\n",
    "  model_fw_df = df.groupby([\"user_id\",\"model_nm\"]).fwver.unique().apply(len).reset_index()\n",
    "  model_fw_df = model_fw_df.groupby(\"user_id\").fwver.max().reset_index()\n",
    "  model_fw_df.columns = [\"user_id\",\"most_freq_model_fw_cnt\"]\n",
    "\n",
    "  source_df = pd.merge(source_df, fwver_cnt_df, on = ['user_id'], how = 'left')\n",
    "  source_df = pd.merge(source_df, model_cnt_df, on = ['user_id'], how = 'left')\n",
    "  source_df = pd.merge(source_df, model_fw_df, on = ['user_id'], how = 'left')\n",
    "  return source_df\n",
    "\n",
    "\n",
    "train_prob =  pd.read_csv(RAW_PATH + \"/train_problem_data.csv\")\n",
    "\n",
    "rst_train1 = get_model_fw_unique_cnt(train_err, True)\n",
    "rst_train2, model_ratio = get_complain_ratio_model(train_err, train_prob, True)\n",
    "rst_train3, fw_ratio = get_complain_ratio_fw(train_err, train_prob, True)\n",
    "\n",
    "\n",
    "rst_test1 = get_model_fw_unique_cnt(test_err, False)\n",
    "rst_test2 = get_complain_ratio_model(test_err, train_prob,False)\n",
    "rst_test3 = get_complain_ratio_fw(test_err, train_prob,False)\n",
    "rst_train2  = pd.merge(rst_train2, model_ratio, on = [\"model_nm\"], how = 'left')\n",
    "rst_test2  = pd.merge(rst_test2, model_ratio, on = [\"model_nm\"], how = 'left')\n",
    "\n",
    "for col in set(rst_train3.columns).difference(rst_test3.columns):\n",
    "  rst_test3[col] = 0\n",
    "\n",
    "for col in set(rst_test3.columns).difference(rst_train3.columns):\n",
    "  rst_train3[col] = 0\n",
    "\n",
    "\n",
    "rst_test3 = rst_test3[list(rst_train3.columns)]\n",
    "rst_train3  = pd.merge(rst_train3, fw_ratio, on = [\"fwver\"], how = 'left')\n",
    "rst_test3  = pd.merge(rst_test3, fw_ratio, on = [\"fwver\"], how = 'left')\n",
    "rst_train = pd.concat([rst_train1, rst_train2.iloc[:,2:]], axis =1)\n",
    "rst_train = pd.concat([rst_train, rst_train3.iloc[:,2:]], axis =1)\n",
    "source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "rst_test1 = pd.merge(source_df, rst_test1, on = [\"user_id\"], how='left')\n",
    "rst_test2 = pd.merge(source_df, rst_test2, on = [\"user_id\"], how='left')\n",
    "rst_test3 = pd.merge(source_df, rst_test3, on = [\"user_id\"], how='left')\n",
    "rst_test = pd.concat([rst_test1, rst_test2.iloc[:,2:]], axis =1)\n",
    "rst_test = pd.concat([rst_test, rst_test3.iloc[:,2:]], axis =1)\n",
    "rst_test = rst_test.fillna(0)\n",
    "\n",
    "\n",
    "np.save(FEATURE_PATH + \"train_ratio_unique_cnt_features.npy\", rst_train.iloc[:,1:].values)\n",
    "np.save(FEATURE_PATH + \"test_ratio_unique_cnt_features.npy\", rst_test.iloc[:,1:].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_VkS6A1-ClE"
   },
   "source": [
    "에러 타입_코드 별 complain_ratio 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApDQL3TRLm1_"
   },
   "outputs": [],
   "source": [
    "def get_typecode_combi_complain_ratio(df, train_yn, type_code_prob_df):\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  df[\"time_min\"] = df.time.astype(str).str.slice(start=0, stop=12)\n",
    "  df_agg_min = df.groupby([\"user_id\",\"time_min\"]).type_code.apply(lambda x: np.unique(x)).reset_index()\n",
    "  df_agg_min[\"length\"] = df_agg_min.type_code.apply(len)\n",
    "  df_agg_min[\"type_code_str\"] = df_agg_min.type_code.astype(str)\n",
    "  df_agg_min = pd.merge(df_agg_min, source_df, how = 'left', on = [\"user_id\"])\n",
    "  df_agg_min = pd.merge(df_agg_min, type_code_prob_df, on = [\"type_code_str\"], how='left')\n",
    "  return df_agg_min.groupby(\"user_id\").problem_ratio.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3qqKm7Lpmxj"
   },
   "outputs": [],
   "source": [
    "# train_err, train_prob기준으로 100회 이상 등장 & 문제 제기 유저의 0.8이상에서 등장했던 로그 목록을 선별\n",
    "train_err[\"time_min\"] = list(map(lambda x : str(x)[:12],train_err.time.values))\n",
    "df_agg_min = train_err.groupby([\"user_id\",\"time_min\"]).type_code.apply(lambda x: np.unique(x)).reset_index()\n",
    "train_prob_ = train_prob\n",
    "train_prob_['time_min'] = list(map(lambda x : str(x)[:12],train_prob_.time.values))\n",
    "train_prob_['problem'] = 1\n",
    "train_prob_ = train_prob_.drop_duplicates().reset_index()\n",
    "tmp = pd.merge(df_agg_min, train_prob_, on = [\"user_id\"], how = 'left').fillna(0)[['type_code','problem']]\n",
    "tmp['type_code'] = tmp.type_code.astype(str)\n",
    "tmp_cnt = tmp.groupby(\"type_code\").problem.count().reset_index(name = \"cnt\")\n",
    "tmp_problem = tmp.groupby(\"type_code\").problem.apply(lambda x : np.sum(list(x))/len(list(x) ) ).reset_index(name = \"problem_ratio\")\n",
    "tmp_problem = pd.merge(tmp_problem,tmp_cnt, on ='type_code', how = 'left')\n",
    "tmp_problem_ = tmp_problem.query(\"cnt > 100 and problem_ratio > 0.8\").sort_values(\"problem_ratio\", ascending = False)\n",
    "tmp_problem_ = tmp_problem_[tmp_problem_.type_code.apply(lambda x : len(x)> 9)].reset_index()[['type_code','problem_ratio']]\n",
    "tmp_problem_.columns = [\"type_code_str\", \"problem_ratio\"]\n",
    "type_code_prob_df = tmp_problem_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSjAjjSuQYrR"
   },
   "outputs": [],
   "source": [
    "train_eq_tp_cd_prob = get_typecode_combi_complain_ratio(train_err, True, type_code_prob_df)\n",
    "test_eq_tp_cd_prob = get_typecode_combi_complain_ratio(test_err, False, type_code_prob_df)\n",
    "train_eq_tp_cd_prob.to_pickle(FEATURE_PATH + \"train_eq_tp_cd_prob.pkl\")\n",
    "test_eq_tp_cd_prob.to_pickle(FEATURE_PATH + \"test_eq_tp_cd_prob.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBDB5p0xc40J"
   },
   "source": [
    "### problem 다발 동시 동초 로그 카운트\n",
    "> 1. 동시 동초 발생 에러 타입 + 코드 조합 중 problem을 제기하지 않은 유저에 비해 problem을 제기한 유저에서 가장 많이 등장한 로그 조합을 뽑음  \n",
    "> 2. 이 로그조합들에 얼마나 해당되었는지 카운트  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY39_3l-c40U"
   },
   "outputs": [],
   "source": [
    "# problem 유저에게서 더 빈번하게 발생한 동시 발생 에러 타입 +코드 조합 상위 40개를 추출하는 과정 \n",
    "train_quality = pd.merge(train_quality, train_prob[[\"user_id\",\"problem\"]], on =[\"user_id\"], how = 'left').fillna(0)\n",
    "problem_users = train_quality.query(\"problem == 1\").groupby(\"user_id\").count().reset_index().user_id.values\n",
    "normal_users = train_quality.query(\"problem == 0\").groupby(\"user_id\").count().reset_index().user_id.values\n",
    "tmp = train_err[train_err.user_id.isin(normal_users[:15000])].groupby([\"user_id\",\"time\"]).type_code.apply(lambda x : str(np.sort(list(x))) if len(x) > 1 else 0).reset_index()\n",
    "tmp2 = train_err[train_err.user_id.isin(problem_users[:15000])].groupby([\"user_id\",\"time\"]).type_code.apply(lambda x : str(np.sort(list(x))) if len(x) > 1 else 0).reset_index()\n",
    "\n",
    "p_rst = tmp2.query(\"type_code != 0\").groupby(\"type_code\").count().sort_values(\"user_id\").reset_index()\n",
    "n_rst = tmp.query(\"type_code != 0\").groupby(\"type_code\").count().sort_values(\"user_id\").reset_index()\n",
    "n_rst = n_rst[['type_code','user_id']]\n",
    "n_rst.columns  = ['type_code','normal_cnt']\n",
    "p_rst = p_rst[['type_code','user_id']]\n",
    "p_rst.columns  = ['type_code','problem_cnt']\n",
    "all_rst = pd.merge(n_rst,p_rst, on = [\"type_code\"], how = 'left').fillna(0)\n",
    "all_rst['p_ratio']= all_rst['problem_cnt']/all_rst['normal_cnt']\n",
    "problem_simul_list = all_rst.sort_values(\"p_ratio\", ascending=False).head(40)\n",
    "# problem_simul_list.to_pickle(\"problem_simul_list.pkl\")\n",
    "# problem_simul_list = pd.read_pickle(\"problem_simul_list.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD_AVQ5Mc40U"
   },
   "outputs": [],
   "source": [
    "def get_simul_prob_top_cnt(df, train_yn, problem_simul_list):\n",
    "  '''\n",
    "  유저별로 사전에 추출한 에러 타입 + 코드 조합 40개에 얼마나 해당되었는지 카운트하는 함수\n",
    "  '''\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "\n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "  train_pr_si = df.groupby([\"user_id\",\"time\"]).type_code.apply(lambda x : str(np.sort(list(x))) if len(x) > 1 else 0).reset_index()\n",
    "  train_pr_si_ = train_pr_si.query(\"type_code != 0 \")\n",
    "  train_pr_si_ = train_pr_si_[train_pr_si_.type_code.isin(list(problem_simul_list.type_code))]\n",
    "  target_df = train_pr_si_.groupby(\"user_id\").count().reset_index()[['user_id','time']].reset_index(drop = True)\n",
    "  target_df.columns = [\"user_id\",\"simul_prob_top_cnt\"]\n",
    "  return pd.merge(source_df, target_df, on =[\"user_id\"], how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoKAFSSfc40U"
   },
   "outputs": [],
   "source": [
    "train_simul_prob_top_cnt = get_simul_prob_top_cnt(train_err, True, problem_simul_list)\n",
    "test_simul_prob_top_cnt = get_simul_prob_top_cnt(test_err, False, problem_simul_list)\n",
    "train_simul_prob_top_cnt.to_pickle(FEATURE_PATH + \"train_simul_prob_top_cnt.pkl\")\n",
    "test_simul_prob_top_cnt.to_pickle(FEATURE_PATH + \"test_simul_prob_top_cnt.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFwP_aUWWyQW"
   },
   "source": [
    "### 퀄리티에 대한 timestamp 분포 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvuMx_n96DRc"
   },
   "outputs": [],
   "source": [
    "def get_quality_timestamp_features(df, train_yn):  \n",
    "  df[\"timestamp\"] = df.time.apply(lambda x : convert_unixtime(str(x)))\n",
    "  df[\"time_day\"] = list(map(lambda x : str(x)[:8], df.time.values))\n",
    "  \n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "  source_df[\"all_day_list\"] = [all_day_list]*len(source_df)\n",
    "  source_df = source_df.explode(\"all_day_list\")\n",
    "  source_df.columns = [\"user_id\", \"time_day\"]\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "         'quality_5', 'quality_6', 'quality_7',\n",
    "        'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "        \n",
    "  # 유저별 time_day 별 source_df 를 만들고 퀄리티별 아래의 통계정보를 붙이기 마지막 fillna처리\n",
    "  # timeday마다의 6*11개 통계피쳐 추출 \n",
    "  import tqdm\n",
    "  for qlt_nm in tqdm.tqdm(cols):\n",
    "    tmp_df = train_quality[['user_id',qlt_nm,'time_day','timestamp']]\n",
    "    tmp_df = tmp_df.query(\"{} != 0\".format(qlt_nm)).reset_index()\n",
    "\n",
    "    agg_df = tmp_df.groupby([\"user_id\",\"time_day\"]).timestamp.describe().reset_index()\n",
    "    agg_df['time_min_max_interval'] = agg_df['max'] - agg_df['min']\n",
    "    agg_df['count/time_min_max_interval'] = agg_df['count']/agg_df['time_min_max_interval']\n",
    "    agg_df = agg_df.replace([np.inf, -np.inf], np.nan)\n",
    "    agg_df = agg_df.reset_index().fillna(0)[['user_id','time_day','mean','std','min','max','time_min_max_interval', 'count/time_min_max_interval']]\n",
    "    agg_df.columns = list(agg_df.columns[:2]) + [i + str(qlt_nm) for i in agg_df.columns[2:]  ]\n",
    "    source_df = pd.merge(source_df, agg_df, on = [\"user_id\",\"time_day\"], how  = 'left')\n",
    "  return source_df.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1100948,
     "status": "ok",
     "timestamp": 1612190158299,
     "user": {
      "displayName": "yang jo",
      "photoUrl": "",
      "userId": "01079575632044760993"
     },
     "user_tz": -540
    },
    "id": "Yg6AAywc8W8a",
    "outputId": "f03c4a59-eb02-4248-d6f3-f1f7d988ef7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [08:57<00:00, 48.85s/it]\n",
      "100%|██████████| 11/11 [08:52<00:00, 48.41s/it]\n"
     ]
    }
   ],
   "source": [
    "train_quality_timestamp_features = get_quality_timestamp_features(train_quality, True)\n",
    "train_quality_timestamp_features.to_pickle(FEATURE_PATH + \"train_quality_timestamp_features.pkl\")\n",
    "test_quality_timestamp_features = get_quality_timestamp_features(test_quality, False)\n",
    "test_quality_timestamp_features.to_pickle(FEATURE_PATH +  \"test_quality_timestamp_features.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LNrkYkTW_99"
   },
   "source": [
    "### 퀄리티 변수 -1값 카운트 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AhcPVZiCclM"
   },
   "outputs": [],
   "source": [
    "# 각 유저별 퀄리티 변수에 대한 -1값을 카운트 하는 함수\n",
    "def get_quality_negative_value_cnt(df, train_yn):\n",
    "  quality_columns  = ['quality_0', 'quality_1', 'quality_2', 'quality_5', 'quality_6', 'quality_7', 'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "\n",
    "  def f(x):\n",
    "    d = {}\n",
    "    idx_nm = []\n",
    "    for q in quality_columns:\n",
    "      d[q + \"_minus_ratio\"] = np.sum(x[q] == -1)/np.sum(x[q] != 0)\n",
    "      idx_nm.append(q + \"_minus_ratio\")\n",
    "    return pd.Series(d, index=idx_nm)\n",
    "\n",
    "  agg_quality = df.groupby(\"user_id\").apply(f).reset_index()\n",
    "  agg_quality = agg_quality.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  \n",
    "  return pd.merge(source_df, agg_quality, on = [\"user_id\"], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40hZi8w9DHU3"
   },
   "outputs": [],
   "source": [
    "train_quality_negaive_value_cnt = get_quality_negative_value_cnt(train_quality, True)\n",
    "train_quality_negaive_value_cnt.to_pickle(FEATURE_PATH + \"train_quality_negaive_value_cnt.pkl\")\n",
    "test_quality_negaive_value_cnt = get_quality_negative_value_cnt(test_quality, False)\n",
    "test_quality_negaive_value_cnt.to_pickle(FEATURE_PATH + \"test_quality_negaive_value_cnt.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQTAb3ztXJUt"
   },
   "source": [
    "### 퀄리티 로그 동시 발생 카운트 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NXAUIahWy3m"
   },
   "outputs": [],
   "source": [
    "def get_simultaneous_quality_features(df, train_yn):\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "         'quality_5', 'quality_6', 'quality_7',\n",
    "        'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "  simul_sum = df[cols].apply(lambda x : np.abs(np.sign(x))).sum(axis =1)\n",
    "  df = df[['user_id']]\n",
    "  df['simul_sum']  = simul_sum\n",
    "  r1 = df.groupby(\"user_id\").simul_sum.describe()[['count','mean','std','max']].reset_index()\n",
    "  r2 = df.groupby(\"user_id\").simul_sum.sum().reset_index()\n",
    "  agg_all = pd.merge(r1,r2, on = [\"user_id\"], how ='left')\n",
    "  rst_df = pd.merge(source_df, agg_all, on  = [\"user_id\"], how = 'left').fillna(0)\n",
    "  rst_df.columns = [\"user_id\",\"simul_cnt_all\",\"simul_cnt_mean\",\"simul_cnt_std\",\"simul_cnt_max\",\"simul_cnt_sum\"]\n",
    "  return rst_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOLBNsjtOkPs"
   },
   "outputs": [],
   "source": [
    "train_simultaneous_quality_features = get_simultaneous_quality_features(train_quality, True)\n",
    "train_simultaneous_quality_features.to_pickle(FEATURE_PATH + \"train_simultaneous_quality_features.pkl\")\n",
    "test_simultaneous_quality_features = get_simultaneous_quality_features(test_quality, False)\n",
    "test_simultaneous_quality_features.to_pickle(FEATURE_PATH + \"test_simultaneous_quality_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84ch7zynXwVT"
   },
   "source": [
    "퀄리티/에러 변수의 유니크 값에 대한 카운트 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUjvdfP6WH3p"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_errtype_quality_sum(err_df, qlt_df, train_yn):\n",
    "  '''\n",
    "  유저별 각 퀄리티 변수합 / 에러 타입별 발생횟수 계산\n",
    "  '''\n",
    "  errtype_df = err_df.groupby([\"user_id\",\"errtype\"]).count().reset_index()[[\"user_id\",\"errtype\",\"time\"]]\n",
    "  errtype_df.columns = ['user_id', 'errtype', 'err_cnt']\n",
    "  def f(x):\n",
    "      d = {}\n",
    "      cols = []\n",
    "      \n",
    "      for i in [0,1,2,5,6,7,8,9,10,11,12]:\n",
    "        col = \"quality_{}\".format(i)\n",
    "        col_ = col + \"_agg_sum\"\n",
    "        cols.append(col_)\n",
    "        d[col_] = np.sum(x[col])\n",
    "      \n",
    "      return pd.Series(d, index=cols)\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "    \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "          'quality_5', 'quality_6', 'quality_7',\n",
    "        'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "\n",
    "  qlt_df[\"time_day\"] = list(map(lambda x : str(x)[:8], qlt_df.time.values))\n",
    "  qlt_simul_sum = qlt_df.groupby([\"user_id\"]).apply(f).reset_index()\n",
    "  \n",
    "  source_df[\"errtype\"] = [np.sort(err_df.errtype.unique())]*user_size\n",
    "  source_df = source_df.explode(\"errtype\")\n",
    "  source_df.columns = [\"user_id\",\"errtype\"]\n",
    "  source_df = pd.merge(source_df, errtype_df, on = [\"user_id\",\"errtype\"], how = \"left\" )\n",
    "  \n",
    "  source_df = source_df.fillna(0)\n",
    "  source_df = pd.merge(source_df , qlt_simul_sum, on = [\"user_id\"], how = 'left').fillna(0)\n",
    "  tmp  = source_df.loc[:, \"quality_0_agg_sum\":\"quality_12_agg_sum\"]//source_df[['err_cnt']].values\n",
    "  tmp = tmp.replace([np.inf, -np.inf], 0)\n",
    "  tmp = tmp.fillna(0)\n",
    "  return pd.concat([source_df[['user_id','errtype']], tmp], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u399a5HRZxNI"
   },
   "outputs": [],
   "source": [
    "train_errtype_quality_sum = get_errtype_quality_sum(train_err, train_quality, True)\n",
    "test_errtype_quality_sum = get_errtype_quality_sum(test_err, test_quality, False)\n",
    "\n",
    "train_errtype_quality_sum.to_pickle(FEATURE_PATH + \"train_errtype_quality_sum.pkl\")\n",
    "test_errtype_quality_sum.to_pickle(FEATURE_PATH + \"test_errtype_quality_sum.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCyB_YXEhMck"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_err_qlt_uniq_feature(err_df, qlt_df, train_yn):\n",
    "  '''\n",
    "  유저별 각 퀄리티 변수합 / 에러 타입별 발생횟수 계산\n",
    "  '''\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "    \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "  \n",
    "  # 유저별 에러타입 유니크 수 계산\n",
    "  err_df_unq = err_df.groupby(\"user_id\").errtype.unique().apply(len).reset_index()\n",
    "  err_df_unq.columns  = [\"user_id\",\"unique_errtype\"]\n",
    "\n",
    "  # 유저별 0이 아닌 퀄리티 발생 유니크 수 계산\n",
    "  cols = ['quality_0', 'quality_1', 'quality_2',\n",
    "          'quality_5', 'quality_6', 'quality_7',\n",
    "        'quality_8', 'quality_9', 'quality_10', 'quality_11', 'quality_12']\n",
    "  qlt_df_ = qlt_df[[\"user_id\"]+cols].groupby(\"user_id\").sum().reset_index()\n",
    "  simul_sum = qlt_df_[cols].apply(lambda x : np.abs(np.sign(x))).sum(axis =1)\n",
    "\n",
    "  qlt_df_ = qlt_df_[['user_id']] \n",
    "  qlt_df_[\"unique_quality\"] = simul_sum\n",
    "\n",
    "  source_df = pd.merge(source_df, err_df_unq, on = [\"user_id\"], how = 'left')\n",
    "  source_df = pd.merge(source_df, qlt_df_, on = [\"user_id\"], how = 'left')\n",
    "  source_df[\"unique_quality/unique_errtype\"] = source_df[\"unique_quality\"] / source_df[\"unique_errtype\"] \n",
    "  return source_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3RfanEGjEL7"
   },
   "outputs": [],
   "source": [
    "# unique 피쳐 추가로 가공하는 과정\n",
    "train_err_qlt_uniq_feature = get_err_qlt_uniq_feature(train_err, train_quality, True)\n",
    "test_err_qlt_uniq_feature = get_err_qlt_uniq_feature(test_err, test_quality, False)\n",
    "use_cols = ['quality_0_agg_sum', 'quality_1_agg_sum',\n",
    "      'quality_2_agg_sum', 'quality_5_agg_sum', 'quality_6_agg_sum',\n",
    "      'quality_7_agg_sum', 'quality_8_agg_sum', 'quality_9_agg_sum',\n",
    "      'quality_10_agg_sum', 'quality_11_agg_sum', 'quality_12_agg_sum']\n",
    "train_agg_quality_sum =  train_errtype_quality_sum.groupby(\"user_id\")[use_cols].sum().reset_index()\n",
    "train_err_qlt_uniq_feature = pd.merge(train_err_qlt_uniq_feature, train_agg_quality_sum, on = 'user_id', how = 'left')\n",
    "for c in use_cols :\n",
    "  train_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"]  = train_err_qlt_uniq_feature[c] / train_err_qlt_uniq_feature[\"unique_errtype\"] \n",
    "  train_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"] = train_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"].fillna(0)\n",
    "fin_use_cols = [\"user_id\"]+ list(filter(lambda x: x.find(\"/\") != -1, train_err_qlt_uniq_feature) )\n",
    "train_err_qlt_uniq_feature = train_err_qlt_uniq_feature[fin_use_cols]\n",
    "test_agg_quality_sum =  test_errtype_quality_sum.groupby(\"user_id\")[use_cols].sum().reset_index()\n",
    "test_err_qlt_uniq_feature = pd.merge(test_err_qlt_uniq_feature, test_agg_quality_sum, on = 'user_id', how = 'left')\n",
    "for c in use_cols :\n",
    "  test_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"]  = test_err_qlt_uniq_feature[c] / test_err_qlt_uniq_feature[\"unique_errtype\"] \n",
    "  test_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"] = test_err_qlt_uniq_feature[  c + \"/\" + \"unique_errtype\"].fillna(0)\n",
    "fin_use_cols = [\"user_id\"]+ list(filter(lambda x: x.find(\"/\") != -1, test_err_qlt_uniq_feature) )\n",
    "test_err_qlt_uniq_feature = test_err_qlt_uniq_feature[fin_use_cols]\n",
    "train_err_qlt_uniq_feature.to_pickle(FEATURE_PATH + \"train_err_qlt_uniq_feature.pkl\")\n",
    "test_err_qlt_uniq_feature.to_pickle(FEATURE_PATH + \"test_err_qlt_uniq_feature.pkl\")\n",
    "train_errtype_quality_sum.to_pickle(FEATURE_PATH + \"train_errtype_quality_sum.pkl\")\n",
    "test_errtype_quality_sum.to_pickle(FEATURE_PATH + \"test_errtype_quality_sum.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbNeH8mK9NrD"
   },
   "source": [
    "### fPCA 추세 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1N5U0Nkkr4Z"
   },
   "outputs": [],
   "source": [
    "def get_quality_agg_cnt_hours(df, train_yn):\n",
    "  '''\n",
    "  유저별 시간별 퀄리티 컬럼 별 cnt 값 생성(33days * 24hours * 11 qualities)\n",
    "  '''\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "\n",
    "  \n",
    "  def f(x):\n",
    "      d = {}\n",
    "      cols = []\n",
    "      \n",
    "      for i in [0,1,2,5,6,7,8,9,10,11,12]:\n",
    "        col = \"quality_{}\".format(i)\n",
    "        col_ = col + \"_agg_sum\"\n",
    "        cols.append(col_)\n",
    "        d[col_] = np.sum(x[col])\n",
    "      \n",
    "      return pd.Series(d, index=cols)\n",
    "\n",
    "  df[\"time_hour\"] = df.time.astype(str).str.slice(start=0, stop=10, step=1)\n",
    "  df[\"time_day\"] = df.time.astype(str).str.slice(start=0, stop=8, step=1)\n",
    "  df_agg_time_day = df.groupby([\"user_id\",\"time_hour\"]).apply(f)\n",
    "  source_df[\"time_hour\"] =[all_hour_list]*len(source_df)\n",
    "  source_df = source_df.explode(\"time_hour\")\n",
    "  target_df = df_agg_time_day.reset_index()\n",
    "  source_df = pd.merge(source_df, target_df, on = [\"user_id\",\"time_hour\"], how = 'left').fillna(0)\n",
    "  source_df = source_df.sort_values([\"user_id\",\"time_hour\"]).reset_index(drop = True)\n",
    "  return source_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fToq2gxiEf75"
   },
   "outputs": [],
   "source": [
    "# 유저별 일별, 시간별 레코드 생성을(explode 처리) 위한 all day list, all time list 생성\n",
    "\n",
    "all_hour_list = []\n",
    "for day in all_day_list:\n",
    "  tmp = [day + i for i in [\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"] ]\n",
    "  all_hour_list.append(tmp)\n",
    "all_hour_list = sum( all_hour_list, [])\n",
    "\n",
    "\n",
    "# 유저별 일별/시간별 fpca 추출\n",
    "train_fpca_quality_hours = get_quality_agg_cnt_hours(train_quality, True)\n",
    "train_fpca_quality_hours.iloc[:,1:].to_csv(INTERMEDIATE_PATH + \"train_fpca_quality_hours.tsv\", sep = '\\t', index = False, header = True )\n",
    "test_fpca_quality_hours = get_quality_agg_cnt_hours(test_quality, False)\n",
    "test_fpca_quality_hours.iloc[:,1:].to_csv(INTERMEDIATE_PATH + \"test_fpca_quality_hours.tsv\", sep = '\\t', index = False, header = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkUqFuADW7lr"
   },
   "source": [
    "### Datetime 관련 피쳐 추출  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rze2TS2YEQmM"
   },
   "outputs": [],
   "source": [
    "def get_err_day_dummy_variable(df, train_yn):\n",
    "  '''\n",
    "  유저별 에러로그 발생시각이 새벽을 제외한 시각인지 카운트 피쳐 생성\n",
    "  토,일,월요일 기준 얼마나 발생하였는지 카운트 피쳐 생성\n",
    "  '''\n",
    "  df[\"datetime\"] = df['time'].apply(make_datetime)\n",
    "  df[\"weekday\"] = df[\"datetime\"].apply(lambda x : x.weekday())\n",
    "  df[\"hour\"] = df[\"datetime\"].apply(lambda x : x.hour)\n",
    "\n",
    "  hour_df = df.query(\"hour >=10 and hour <= 23\").reset_index(drop = True)\n",
    "  hour_df[\"hour_cnt\"] = 1\n",
    "  agg_hour_df = hour_df.groupby(\"user_id\").hour_cnt.sum().reset_index(name=\"activity_hour_cnt\")\n",
    "  df['weekday_cnt'] = 1\n",
    "  agg_weekday_df = df.groupby([\"user_id\",\"weekday\"]).weekday_cnt.sum().reset_index(name=\"weekday_cnt_all\")\n",
    "\n",
    "  agg_weekday_df = agg_weekday_df.pivot('weekday', columns='user_id').T.reset_index().fillna(0)\n",
    "  agg_weekday_df[\"all_cnt\"] = agg_weekday_df[[0,1,2,3,4,5,6]].sum(axis= 1)\n",
    "  agg_weekday_df = agg_weekday_df.loc[:,\"user_id\":\"all_cnt\"]\n",
    "  agg_weekday_df.columns = ['user_id', \"weekday_0_cnt\", \"weekday_1_cnt\", \"weekday_2_cnt\", \"weekday_3_cnt\", \"weekday_4_cnt\", \"weekday_5_cnt\", \"weekday_6_cnt\", 'all_cnt']\n",
    "  for col in [\"weekday_0_cnt\", \"weekday_1_cnt\", \"weekday_2_cnt\", \"weekday_3_cnt\", \"weekday_4_cnt\", \"weekday_5_cnt\", \"weekday_6_cnt\"]:\n",
    "    agg_weekday_df[col] = agg_weekday_df[col]/agg_weekday_df[\"all_cnt\"]\n",
    "\n",
    "\n",
    "  if train_yn == True:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(10000, 25000)]})\n",
    "  \n",
    "  else:\n",
    "    source_df = pd.DataFrame({\"user_id\" : [i for i in range(30000, 44999)]})\n",
    "  user_size = len(source_df)\n",
    "\n",
    "  rst_df = pd.merge(source_df, agg_hour_df, on = [\"user_id\"], how = 'left')\n",
    "  rst_df = pd.merge(rst_df, agg_weekday_df, on = [\"user_id\"], how = 'left')\n",
    "  return rst_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZzHNOK-G-Pd"
   },
   "outputs": [],
   "source": [
    "# 추가 전처리 코드\n",
    "train_err_day_dummy_variable = get_err_day_dummy_variable(train_err, True)\n",
    "test_err_day_dummy_variable = get_err_day_dummy_variable(test_err, False)\n",
    "# train_err_day_dummy_variable.to_pickle(FEATURE_PATH + \"train_err_day_dummy_variable.pkl\")\n",
    "# test_err_day_dummy_variable.to_pickle(FEATURE_PATH + \"test_err_day_dummy_variable.pkl\")\n",
    "\n",
    "train_err_day_dummy_variable[\"active_hour_ratio\"] = train_err_day_dummy_variable['activity_hour_cnt']/train_err_day_dummy_variable['all_cnt']\n",
    "test_err_day_dummy_variable[\"active_hour_ratio\"] = test_err_day_dummy_variable['activity_hour_cnt']/test_err_day_dummy_variable['all_cnt']\n",
    "\n",
    "train_err_day_dummy_variable[['user_id', 'weekday_0_cnt', 'weekday_1_cnt',\n",
    "       'weekday_2_cnt', 'weekday_3_cnt', 'weekday_4_cnt', 'weekday_5_cnt',\n",
    "       'weekday_6_cnt', 'active_hour_ratio']].to_pickle(FEATURE_PATH + \"train_err_day_dummy_variable.pkl\")\n",
    "\n",
    "test_err_day_dummy_variable[['user_id', 'weekday_0_cnt', 'weekday_1_cnt',\n",
    "       'weekday_2_cnt', 'weekday_3_cnt', 'weekday_4_cnt', 'weekday_5_cnt',\n",
    "       'weekday_6_cnt', 'active_hour_ratio']].to_pickle(FEATURE_PATH + \"test_err_day_dummy_variable.pkl\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
